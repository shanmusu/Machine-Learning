{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import Counter\n",
    "from TwitterAPI import TwitterAPI\n",
    "from collections import Counter, defaultdict, deque\n",
    "import csv\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def open_file(tweets,fname):\n",
    "    fname=fname+'.pkl'\n",
    "    p = Path(fname)\n",
    "    if p.is_file():\n",
    "        with open(fname, \"rb\") as file:\n",
    "            try:\n",
    "                tweets = pickle.load(file)\n",
    "            except EOFError:\n",
    "                return tweets\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tweets(file,tweets_list):\n",
    "    for i in file:\n",
    "        t=[]\n",
    "        tweets_list.append(i['tweet'])\n",
    "    return tweets_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_Data(allTweet):\n",
    "    data = []\n",
    "    for json in allTweet:\n",
    "        tweet_data = []\n",
    "        tweet_data.append(json['username'])\n",
    "        tweet_data.append(json['userid'])\n",
    "        tweet_data.append(json['description'])\n",
    "        tweet_data.append(json['tweet'])\n",
    "       \n",
    "        data.append(tweet_data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_tweets_csv(data):\n",
    "    with open('twitter_data.csv', 'w',encoding='utf-8') as fp:\n",
    "        a = csv.writer(fp)\n",
    "        a.writerows(data)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def afinn_sentiment2(terms, afinn, verbose=False):\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    val=0\n",
    "    for t in terms:\n",
    "        if t in afinn:\n",
    "            if verbose:\n",
    "                print('\\t%s=%d' % (t, afinn[t]))\n",
    "            if afinn[t] > 0:\n",
    "                pos += afinn[t]\n",
    "                \n",
    "            else:\n",
    "                neg += -1 * afinn[t]\n",
    "                \n",
    "    val=pos+neg\n",
    "    \n",
    "    if val < 0 :\n",
    "        return  -1\n",
    "    else:\n",
    "        return 1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_for_manual(tweet_test,afinn):\n",
    "    tweet_manual_labelling = []\n",
    "    for i in tweet_test:\n",
    "        i = re.sub('http\\S+', 'THIS_IS_A_URL', i)\n",
    "        i = re.sub('@\\S+', 'THIS_IS_A_MENTION', i)\n",
    "        if i.split()[0] != 'RT':\n",
    "            score= afinn_sentiment2(i, afinn, verbose=False)\n",
    "            \n",
    "            tweet_manual_labelling.append((score,i))\n",
    "    with open('tweet_manual_labelling.csv', 'w',encoding='utf-8') as fp1:\n",
    "        filewriter = csv.writer(fp1)\n",
    "        filewriter.writerows(tweet_manual_labelling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_classification(logistic_prediction,tweets_list):\n",
    "    \n",
    "    tweet_classified_labelling = []\n",
    "    for i in range(len(tweets_list)):\n",
    "        tweet_classified_labelling.append((logistic_prediction[i],tweets_list[i]))\n",
    "   \n",
    "    with open('tweet_classified.csv', 'w',encoding='utf-8',newline='') as fp1:\n",
    "        filewriter = csv.writer(fp1)\n",
    "        filewriter.writerows(tweet_classified_labelling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_tarining_data(filename):\n",
    "    tweets = []\n",
    "    labels1 = []\n",
    "    with open(filename, 'r',encoding='utf-8') as x:\n",
    "        filereader = csv.reader(x)\n",
    "        for  row in filereader:\n",
    "            labels1.append(row[0])\n",
    "            tweets.append(row[1])\n",
    "    return tweets,np.array(labels1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \n",
    "    tokens = re.findall(r\"\\w+|\\S\", text.lower(),flags = re.L)\n",
    "    tokens1 = []\n",
    "    for i in tokens:\n",
    "        i = re.sub('http\\S+', 'THIS_IS_A_URL', i)\n",
    "        i = re.sub('@\\S+', 'THIS_IS_A_MENTION', i)\n",
    "        x = re.findall(r\"\\w+|\\S\", i,flags = re.U)\n",
    "        for j in x:\n",
    "            tokens1.append(j)            \n",
    "    return tokens1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_vectorize(tokenizer_fn=tokenize, min_df=1,\n",
    "                 max_df=1., binary=False, ngram_range=(1,1)):\n",
    "\n",
    "    \n",
    "    vectorizer = CountVectorizer(input = 'content', tokenizer = tokenizer_fn, min_df=min_df, \n",
    "                                     max_df=max_df, binary=True, ngram_range=ngram_range,\n",
    "                                 dtype = 'int',analyzer='word',token_pattern='(?u)\\b\\w\\w+\\b',encoding='utf-8' )\n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_clf():\n",
    "    return LogisticRegression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prediction(CLF,trained_CSR,trained_label,untrained_tweets_CSR):\n",
    "    CLF.fit(trained_CSR,trained_label)\n",
    "    predicted = CLF.predict(untrained_tweets_CSR)\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_cross_validation(X, y,clf, n_folds=5):\n",
    "    cv = KFold(len(y), n_folds)\n",
    "    accuracies = []\n",
    "    for train_ind, test_ind in cv: \n",
    "        clf.fit(X[train_ind], y[train_ind])\n",
    "        predictions = clf.predict(X[test_ind])\n",
    "        accuracies.append(accuracy_score(y[test_ind], predictions))\n",
    "    avg = np.mean(accuracies)\n",
    "    return avg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_piechart(key,value):\n",
    "    figure(1, figsize=(8,8))\n",
    "    ax = axes([0.1, 0.1, 0.5, 0.5])\n",
    "    if key == -1:\n",
    "        title1 = 'Tweets Aganist Marijuana'\n",
    "    elif key == 0:\n",
    "        title1 = 'Neutral Sentiment Tweets on Marijuana'\n",
    "    elif key == 1:\n",
    "        title1 = 'Pro Marijuana Tweets'\n",
    "    elif key == 2:\n",
    "        title1 = 'Tweets Supporting its Medinical Use'\n",
    "    title(title1, bbox={'facecolor':'0.8', 'pad':5})\n",
    "    explode = [0.1]*len(labels)\n",
    "    pie(value,explode = explode,labels=labels,\n",
    "                autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-176-ea45a41d78cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m#get the vectorizer object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdo_vectorize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlabeled_tweets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mlogistic_regression_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdo_cross_validation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclf_logistic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\pradeep\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    815\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 817\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    819\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\pradeep\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    762\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 764\u001b[1;33m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[0;32m    765\u001b[0m                                  \" contain stop words\")\n\u001b[0;32m    766\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "#,'#DefendTheLand'\n",
    "#hashtags=['#DubNation','#WeTheNorth','#GrindCity','#DetroitBasketball','#thunderup','#LakeShow']\n",
    "hashtags=['#DubNation']\n",
    "tweets_list=[]\n",
    "url = urlopen('http://www2.compute.dtu.dk/~faan/data/AFINN.zip')\n",
    "zipfile = ZipFile(BytesIO(url.read()))\n",
    "afinn_file = zipfile.open('AFINN/AFINN-111.txt')\n",
    "afinn = dict()\n",
    "for line in afinn_file:\n",
    "    parts = line.strip().split()\n",
    "    if len(parts) == 2:\n",
    "        afinn[parts[0].decode(\"utf-8\")] = int(parts[1])   \n",
    "      \n",
    "labeled_tweets,labels = read_tarining_data('tweet_manual_labelling.csv')\n",
    "clf_logistic = get_clf() \n",
    "#get the vectorizer object\n",
    "vectorizer = do_vectorize() \n",
    "X = vectorizer.fit_transform(tweet for tweet in labeled_tweets)\n",
    "y = np.array(labels) \n",
    "logistic_regression_accuracy = (do_cross_validation(X, y,clf_logistic))*100\n",
    "print('Average cross validation accuracy for Logistic Regression=%.1f percentage' % (logistic_regression_accuracy))\n",
    "    \n",
    "for tags in hashtags:\n",
    "    tweets=[]\n",
    "    #get tweets for each team \n",
    "    tweets= open_file(tweets,tags)\n",
    "    #to store in csv process the data\n",
    "    data=process_Data(tweets)\n",
    "    write_tweets_csv(data)    \n",
    "    #data to be tested\n",
    "    tweets_list= get_tweets(tweets,tweets_list)\n",
    "    create_for_manual(tweets_list,afinn) \n",
    "    #Prediction for unlabelled Tweets\n",
    "    test_tweet_vector = vectorizer.transform(t for t in tweets_list)\n",
    "    logistic_prediction =prediction(clf_logistic,X,y,test_tweet_vector)\n",
    "    write_classification(logistic_prediction,tweets_list)\n",
    "    result = dict(Counter(logistic_prediction))\n",
    "    print (\"Logistic Regression Results for team\",tags)\n",
    "    pos=0\n",
    "    neg=0\n",
    "    neu=0\n",
    "    for i in result:\n",
    "        if i == '-1':\n",
    "            print (\"\\t Tweets aganist team\\t\\t\\t\\t\",result[i])\n",
    "            neg+=result[i]\n",
    "            \n",
    "        elif i == '0':\n",
    "            print (\"\\t Number of advertising tweets on team \\t\\t%d\" %result[i])\n",
    "            neu+=result[i]\n",
    "        elif i == '1':\n",
    "            print (\"\\t Number of Tweets supporting team \\t\\t%d\" %result[i])\n",
    "            pos+=result[i]\n",
    "    \n",
    "    labeled_tweets,labels = read_tarining_data('tweet_classified.csv')\n",
    "       \n",
    "    \"\"\"    vals = sorted(result.keys())\n",
    "    counts =\n",
    "\n",
    "    for i in result:\n",
    "        total = pos+neg+neu\n",
    "        per_list = []\n",
    "        for j in user_sentiment_count[i]:\n",
    "        per = (1.*j/total)*100\n",
    "        per_list.append(per)\n",
    "        user_sentiment_percentage[i] = per_list\n",
    "    for t in range(len):\n",
    "   \n",
    "    print_piechart(key,value)\n",
    "        #print (labeled_tweets[t],\"\\t\",labels[t])\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
