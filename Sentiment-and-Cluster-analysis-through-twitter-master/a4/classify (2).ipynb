{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import Counter\n",
    "from TwitterAPI import TwitterAPI\n",
    "from collections import Counter, defaultdict, deque\n",
    "import csv\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def open_file(tweets,fname):\n",
    "    fname=fname+'.pkl'\n",
    "    p = Path(fname)\n",
    "    if p.is_file():\n",
    "        with open(fname, \"rb\") as file:\n",
    "            try:\n",
    "                tweets = pickle.load(file)\n",
    "            except EOFError:\n",
    "                return tweets\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tweets(file,tweets_list):\n",
    "    for i in file:\n",
    "        t=[]\n",
    "        tweets_list.append(i['tweet'])\n",
    "    return tweets_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_Data(allTweet):\n",
    "    data = []\n",
    "    for json in allTweet:\n",
    "        tweet_data = []\n",
    "        tweet_data.append(json['username'])\n",
    "        tweet_data.append(json['userid'])\n",
    "        tweet_data.append(json['description'])\n",
    "        tweet_data.append(json['tweet'])\n",
    "       \n",
    "        data.append(tweet_data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_tweets_csv(data):\n",
    "    with open('twitter_data.csv', 'w',encoding='utf-8') as fp:\n",
    "        a = csv.writer(fp)\n",
    "        a.writerows(data)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def affin():\n",
    "    url = urlopen('http://www2.compute.dtu.dk/~faan/data/AFINN.zip')\n",
    "    zipfile = ZipFile(BytesIO(url.read()))\n",
    "    afinn_file = zipfile.open('AFINN/AFINN-111.txt')\n",
    "    afinn = dict()\n",
    "    for line in afinn_file:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) == 2:\n",
    "            afinn[parts[0].decode(\"utf-8\")] = int(parts[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def afinn_sentiment2(terms, afinn, verbose=False):\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    val=0\n",
    "    for t in terms:\n",
    "        if t in afinn:\n",
    "            if verbose:\n",
    "                print('\\t%s=%d' % (t, afinn[t]))\n",
    "            if afinn[t] > 0:\n",
    "                pos += afinn[t]\n",
    "                \n",
    "            else:\n",
    "                neg += -1 * afinn[t]\n",
    "                \n",
    "    val=pos+neg\n",
    "    \n",
    "    if val < 0 :\n",
    "        return  -1\n",
    "    else:\n",
    "        return 1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_for_manual(tweet_test,afinn):\n",
    "    tweet_manual_labelling = []\n",
    "    for i in tweet_test:\n",
    "        i = re.sub('http\\S+', 'THIS_IS_A_URL', i)\n",
    "        i = re.sub('@\\S+', 'THIS_IS_A_MENTION', i)\n",
    "        if i.split()[0] != 'RT':\n",
    "            score= afinn_sentiment2(i, afinn, verbose=False)\n",
    "            \n",
    "            tweet_manual_labelling.append((score,i))\n",
    "    with open('tweet_manual_labelling.csv', 'w',encoding='utf-8',newline='') as fp1:\n",
    "        filewriter = csv.writer(fp1)\n",
    "        filewriter.writerows(tweet_manual_labelling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_classification(f,logistic_prediction,tweets_list):\n",
    "    \n",
    "    tweet_classified_labelling = []\n",
    "    for i in range(len(tweets_list)):\n",
    "        tweet_classified_labelling.append((logistic_prediction[i],tweets_list[i]))\n",
    "   \n",
    "    with open(f, 'w',encoding='utf-8',newline='') as fp1:\n",
    "        filewriter = csv.writer(fp1)\n",
    "        filewriter.writerows(tweet_classified_labelling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_tarining_data(filename):\n",
    "    tweets = []\n",
    "    labels1 = []\n",
    "    with open(filename, 'r',encoding='utf-8') as x:\n",
    "        filereader = csv.reader(x)\n",
    "        for  row in filereader:\n",
    "            labels1.append(row[0])\n",
    "            tweets.append(row[1])\n",
    "    return tweets,np.array(labels1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \n",
    "    tokens = re.findall(r\"\\w+|\\S\", text.lower(),flags = re.L)\n",
    "    tokens1 = []\n",
    "    for i in tokens:\n",
    "        i = re.sub('http\\S+', 'THIS_IS_A_URL', i)\n",
    "        i = re.sub('@\\S+', 'THIS_IS_A_MENTION', i)\n",
    "        x = re.findall(r\"\\w+|\\S\", i,flags = re.U)\n",
    "        for j in x:\n",
    "            tokens1.append(j)            \n",
    "    return tokens1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_vectorize(tokenizer_fn=tokenize, min_df=1,\n",
    "                 max_df=1., binary=False, ngram_range=(1,1)):\n",
    "\n",
    "    \n",
    "    vectorizer = CountVectorizer(input = 'content', tokenizer = tokenizer_fn, min_df=min_df, \n",
    "                                     max_df=max_df, binary=True, ngram_range=ngram_range,\n",
    "                                 dtype = 'int',analyzer='word',token_pattern='(?u)\\b\\w\\w+\\b',encoding='utf-8' )\n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_clf():\n",
    "    return LogisticRegression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prediction(CLF,trained_CSR,trained_label,untrained_tweets_CSR):\n",
    "    CLF.fit(trained_CSR,trained_label)\n",
    "    predicted = CLF.predict(untrained_tweets_CSR)\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_cross_validation(X, y,clf, n_folds=5):\n",
    "    cv = KFold(len(y), n_folds)\n",
    "    accuracies = []\n",
    "    for train_ind, test_ind in cv: \n",
    "        clf.fit(X[train_ind], y[train_ind])\n",
    "        predictions = clf.predict(X[test_ind])\n",
    "        accuracies.append(accuracy_score(y[test_ind], predictions))\n",
    "    avg = np.mean(accuracies)\n",
    "    return avg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cross validation accuracy for Logistic Regression=84.4 percentage\n",
      "Logistic Regression Results for team #WeTheNorth\n",
      "\t Tweets aganist team\t\t\t\t 87\n",
      "\t Number of Tweets supporting team \t\t1259\n",
      "Logistic Regression Results for team #GrindCity\n",
      "\t Tweets aganist team\t\t\t\t 800\n",
      "\t Number of Tweets supporting team \t\t1637\n",
      "Logistic Regression Results for team #DubNation\n",
      "\t Tweets aganist team\t\t\t\t 836\n",
      "\t Number of Tweets supporting team \t\t2907\n",
      "Logistic Regression Results for team #DefendTheLand\n",
      "\t Tweets aganist team\t\t\t\t 1288\n",
      "\t Number of Tweets supporting team \t\t3684\n",
      "Logistic Regression Results for team #DetroitBasketball\n",
      "\t Tweets aganist team\t\t\t\t 2304\n",
      "\t Number of Tweets supporting team \t\t3895\n",
      "Logistic Regression Results for team #thunderup\n",
      "\t Tweets aganist team\t\t\t\t 3079\n",
      "\t Number of Tweets supporting team \t\t4358\n",
      "Logistic Regression Results for team #LakeShow\n",
      "\t Tweets aganist team\t\t\t\t 3669\n",
      "\t Number of Tweets supporting team \t\t4617\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    hashtags=['#WeTheNorth','#GrindCity','#DubNation','#DefendTheLand','#DetroitBasketball','#thunderup','#LakeShow']\n",
    "    #hashtags=['#DubNation']\n",
    "    tweets_list=[]\n",
    "\n",
    "    labeled_tweets,labels = read_tarining_data('tweet_manual_labelling.csv')\n",
    "    clf_logistic = get_clf() \n",
    "    #get the vectorizer object\n",
    "    vectorizer = do_vectorize() \n",
    "    X = vectorizer.fit_transform(tweet for tweet in labeled_tweets)\n",
    "    y = np.array(labels) \n",
    "    logistic_regression_accuracy = (do_cross_validation(X, y,clf_logistic))*100\n",
    "    print('Average cross validation accuracy for Logistic Regression=%.1f percentage' % (logistic_regression_accuracy))\n",
    "    for tags in hashtags:\n",
    "        tweets=[]\n",
    "        #get tweets for each team \n",
    "        tweets= open_file(tweets,tags)\n",
    "        #to store in csv process the data\n",
    "        data=process_Data(tweets)\n",
    "        write_tweets_csv(data)    \n",
    "        #data to be tested\n",
    "        tweets_list= get_tweets(tweets,tweets_list)\n",
    "        #create_for_manual(tweets_list,afinn) only one -- done for you\n",
    "        #Prediction for unlabelled Tweets\n",
    "        test_tweet_vector = vectorizer.transform(t for t in tweets_list)\n",
    "        logistic_prediction =prediction(clf_logistic,X,y,test_tweet_vector)\n",
    "        fname=tags+ '.csv'\n",
    "        write_classification(fname,logistic_prediction,tweets_list)\n",
    "\n",
    "        result = dict(Counter(logistic_prediction))\n",
    "   \n",
    "\n",
    "\n",
    "        print (\"Logistic Regression Results for team\",tags)\n",
    "        for i in result:\n",
    "            if i == '-1':\n",
    "                print (\"\\t Tweets aganist team\\t\\t\\t\\t\",result[i])\n",
    "\n",
    "            elif i == '0':\n",
    "                print (\"\\t Number of advertising tweets on team \\t\\t%d\" %result[i])\n",
    "\n",
    "            elif i == '1':\n",
    "                print (\"\\t Number of Tweets supporting team \\t\\t%d\" %result[i])\n",
    "\n",
    "        labeled_tweets,labels=read_tarining_data(fname)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
